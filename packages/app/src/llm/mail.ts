import OpenAI from "openai"
import fs from "node:fs"
import path from "node:path"
import { MailLLMReply, MailReplyRecord } from "../type/mail.js"
import { createCallId, errorDetails, isDebugEnabled, logDebugJson, logLine } from "../logging/index.js"

type MailContextMessage = Pick<MailReplyRecord, "role" | "content"> & {
  contactName?: string | null
}

export interface MailLLM {
  generateReply(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    summary: string
    history: MailContextMessage[]
    userInput: string
    logContext?: MailLLMLogContext
  }): Promise<MailLLMReply>

  summarize(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    messages: MailContextMessage[]
    logContext?: MailLLMLogContext
  }): Promise<string>

  createImage(args: {
    prompt: string
    modelQuality: "low" | "normal" | "high"
    logContext?: MailLLMLogContext
  }): Promise<{ mimeType: string; binary: Buffer }>
}

type MailLLMLogContext = {
  threadId?: number | null
  replyId?: number | null
  contactId?: number | null
}

type MailLLMOptions = {
  retryMaxAttempts: number
  requestTimeoutMs: number
  maxAttachments: number
  maxTextAttachmentChars: number
  debug: boolean
}

type MailLLMCreateOptions = {
  retryMaxAttempts?: number
  requestTimeoutMs?: number
  maxAttachments?: number
  maxTextAttachmentChars?: number
  debug?: boolean
}

type OpenAIResponseResult = Awaited<ReturnType<OpenAI["responses"]["create"]>>

const DEV_IMAGE_PATH = new URL("../../data/image.png", import.meta.url)

function safeJsonParse<T>(text: string, fallback: T): T {
  try {
    return JSON.parse(text) as T
  } catch {
    return fallback
  }
}

function normalizeAttachments(
  value: unknown,
  options: {
    maxAttachments: number
    maxTextAttachmentChars: number
  }
): MailLLMReply["attachments"] {
  if (!Array.isArray(value)) return []

  const result: MailLLMReply["attachments"] = []
  for (const candidate of value) {
    if (!candidate || typeof candidate !== "object") continue
    const obj = candidate as Record<string, unknown>
    const kind = obj.kind === "text" || obj.kind === "image" ? obj.kind : null
    if (!kind) continue

    const filename = sanitizeAttachmentFilename(typeof obj.filename === "string" ? obj.filename.trim() : "")
    const modelQuality =
      obj.modelQuality === "low" || obj.modelQuality === "normal" || obj.modelQuality === "high"
        ? obj.modelQuality
        : "normal"

    if (kind === "text") {
      const content = typeof obj.content === "string" ? obj.content.trim() : ""
      if (!filename || !content) continue
      result.push({
        kind,
        filename,
        modelQuality,
        content: content.slice(0, options.maxTextAttachmentChars),
      })
      continue
    }

    const prompt = typeof obj.prompt === "string" ? obj.prompt.trim() : ""
    if (!filename || !prompt) continue
    result.push({
      kind,
      filename,
      modelQuality,
      prompt,
    })
  }

  return result.slice(0, options.maxAttachments)
}

async function wait(ms: number): Promise<void> {
  await new Promise((resolve) => setTimeout(resolve, ms))
}

function randomBetween(minMs: number, maxMs: number): number {
  return Math.floor(Math.random() * (maxMs - minMs + 1)) + minMs
}

function slugify(input: string): string {
  const normalized = input
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "-")
    .replace(/(^-|-$)/g, "")
  return normalized || "file"
}

function sanitizeAttachmentFilename(input: string): string {
  const name = path.basename(input).replace(/[/\\]/g, "").trim()
  if (!name) return "attachment"
  return name.slice(0, 120)
}

function inferDevAttachments(userInput: string): MailLLMReply["attachments"] {
  const normalized = userInput.toLowerCase()
  const attachments: MailLLMReply["attachments"] = []

  if (normalized.includes("create_text")) {
    const base = slugify(userInput).slice(0, 28) || "dummy-text"
    attachments.push({
      kind: "text",
      filename: `${base}.txt`,
      modelQuality: "normal",
      content:
        `Dummy text attachment generated by DevMailLLM for: ${userInput}\n` +
        `- generated_by: DevMailLLM\n` +
        `- trigger: create_text\n` +
        `- purpose: simulate createTextFile tool`,
    })
  }

  if (normalized.includes("create_image")) {
    const base = slugify(userInput).slice(0, 28) || "dummy-image"
    attachments.push({
      kind: "image",
      filename: `${base}.png`,
      modelQuality: "normal",
      prompt: `Dummy image attachment generated by DevMailLLM for: ${userInput}`,
    })
  }

  return attachments.slice(0, 2)
}

function formatLogContext(context?: MailLLMLogContext): string {
  const threadId = context?.threadId ?? "-"
  const replyId = context?.replyId ?? "-"
  const contactId = context?.contactId ?? "-"
  return `thread_id=${threadId} reply_id=${replyId} contact_id=${contactId}`
}

class DevMailLLM implements MailLLM {
  async generateReply(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    summary: string
    history: MailContextMessage[]
    userInput: string
    logContext?: MailLLMLogContext
  }): Promise<MailLLMReply> {
    await wait(randomBetween(700, 1800))

    const preface = args.contactInstruction
      ? `Replying with persona instruction: ${args.contactInstruction}`
      : "Replying with default assistant persona."
    const historyCount = args.history.length
    const hasSummary = Boolean(args.summary.trim())
    const attachments = inferDevAttachments(args.userInput)

    return {
      content: `### Response

${preface}

You said:

> ${args.userInput}

Context:
- History messages considered: ${historyCount}
- Summary available: ${hasSummary ? "yes" : "no"}
- Model: \`${args.model}\`

This reply is generated by \`DevMailLLM\` for local development.`,
      attachments,
    }
  }

  async summarize(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    messages: MailContextMessage[]
    logContext?: MailLLMLogContext
  }): Promise<string> {
    await wait(randomBetween(250, 600))
    const latest = args.messages.slice(-6).map((message) => `${message.role}: ${message.content}`)
    return latest.join("\n\n").slice(0, 1000)
  }

  async createImage(args: {
    prompt: string
    modelQuality: "low" | "normal" | "high"
    logContext?: MailLLMLogContext
  }): Promise<{ mimeType: string; binary: Buffer }> {
    const delayByQuality = args.modelQuality === "high" ? 900 : args.modelQuality === "low" ? 250 : 550
    await wait(delayByQuality)

    return {
      mimeType: "image/png",
      binary: fs.readFileSync(DEV_IMAGE_PATH),
    }
  }
}

class OpenAIMailLLM implements MailLLM {
  openaiClient: OpenAI
  options: MailLLMOptions

  constructor(openaiClient: OpenAI, options: MailLLMOptions) {
    this.openaiClient = openaiClient
    this.options = options
  }

  private async withRetry<T>(run: (attempt: number) => Promise<T>): Promise<T> {
    let lastError: unknown = null
    for (let attempt = 1; attempt <= this.options.retryMaxAttempts; attempt += 1) {
      try {
        return await run(attempt)
      } catch (error) {
        lastError = error
        if (attempt >= this.options.retryMaxAttempts) break
      }
    }
    throw lastError instanceof Error ? lastError : new Error("Mail generation failed")
  }

  private async createWithTimeout(
    args: Parameters<OpenAI["responses"]["create"]>[0]
  ): Promise<OpenAIResponseResult> {
    return await new Promise<OpenAIResponseResult>((resolve, reject) => {
      let settled = false
      const timer = setTimeout(() => {
        settled = true
        reject(new Error(`LLM request timed out after ${this.options.requestTimeoutMs}ms`))
      }, this.options.requestTimeoutMs)

      this.openaiClient.responses.create(args)
        .then((response) => {
          if (settled) return
          settled = true
          clearTimeout(timer)
          resolve(response as OpenAIResponseResult)
        })
        .catch((error) => {
          if (settled) return
          settled = true
          clearTimeout(timer)
          reject(error)
        })
    })
  }

  private getOutputText(response: OpenAIResponseResult): string {
    if ("output_text" in response && typeof response.output_text === "string") {
      return response.output_text
    }
    throw new Error("LLM response did not contain output_text")
  }

  async generateReply(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    summary: string
    history: MailContextMessage[]
    userInput: string
    logContext?: MailLLMLogContext
  }): Promise<MailLLMReply> {
    const historyText = args.history
      .map((message) => {
        const persona = message.contactName ? `(${message.contactName})` : ""
        return `${message.role}${persona}: ${message.content}`
      })
      .join("\n\n")

    try {
      const result = await this.withRetry(async (attempt) => {
        const callId = createCallId()
        const startMs = Date.now()
        try {
          const response = await this.createWithTimeout({
            model: args.model,
            input: [
              {
                role: "system",
                content:
                  `${args.systemPrompt}\n\n` +
                  "Act like a ChatGPT-style assistant: factual, readable, useful, and concise when appropriate. " +
                  "Follow OpenAI content policy. Return JSON only with shape {\"content\": string, \"attachments\": Attachment[]}." +
                  "Attachment for text: {kind:'text', filename, modelQuality, content}. " +
                  "Attachment for image: {kind:'image', filename, modelQuality, prompt}. " +
                  "Create attachments only if the user explicitly asks for a file/image artifact. " +
                  "Reply content must be markdown.",
              },
              {
                role: "system",
                content: `Contact instruction:\n${args.contactInstruction || "(none)"}`,
              },
              {
                role: "system",
                content: `Conversation summary (primary context):\n${args.summary || "(empty)"}`,
              },
              {
                role: "user",
                content:
                  `Recent conversation history:\n${historyText || "(no prior history)"}\n\n` +
                  `Current user message:\n${args.userInput}`,
              },
            ],
          })

          const parsed = safeJsonParse<{ content?: string; attachments?: unknown }>(
            this.getOutputText(response),
            {}
          )
          const content = parsed.content?.trim() || ""
          const attachments = normalizeAttachments(parsed.attachments, {
            maxAttachments: this.options.maxAttachments,
            maxTextAttachmentChars: this.options.maxTextAttachmentChars,
          })

          if (!content) {
            throw new Error("Empty mail reply content")
          }
          const durationMs = Date.now() - startMs
          logLine(
            "info",
            `LLM mail reply-generation ${formatLogContext(args.logContext)} model=${args.model} ok ${durationMs}ms attempt=${attempt} cid=${callId}`
          )
          logDebugJson(this.options.debug, {
            event: "llm.call",
            provider: "openai",
            operation: "responses.create",
            component: "mail",
            trigger: "reply-generation",
            ...args.logContext,
            model: args.model,
            status: "ok",
            durationMs,
            attempt,
            timeoutMs: this.options.requestTimeoutMs,
            callId,
          })
          return { content, attachments }
        } catch (error) {
          const durationMs = Date.now() - startMs
          const details = errorDetails(error)
          logLine(
            "error",
            `LLM mail reply-generation ${formatLogContext(args.logContext)} model=${args.model} error ${durationMs}ms attempt=${attempt} cid=${callId} msg="${details.errorMessage}"`
          )
          logDebugJson(this.options.debug, {
            level: "error",
            event: "llm.call",
            provider: "openai",
            operation: "responses.create",
            component: "mail",
            trigger: "reply-generation",
            ...args.logContext,
            model: args.model,
            status: "error",
            durationMs,
            attempt,
            timeoutMs: this.options.requestTimeoutMs,
            callId,
            errorName: details.errorName,
            errorMessage: details.errorMessage,
          })
          throw error
        }
      })

      return result
    } catch {
      return {
        content: "I could not generate a full response yet.",
        attachments: [],
      }
    }
  }

  async summarize(args: {
    model: string
    systemPrompt: string
    contactInstruction: string
    messages: MailContextMessage[]
    logContext?: MailLLMLogContext
  }): Promise<string> {
    if (args.messages.length === 0) return ""

    const result = await this.withRetry(async (attempt) => {
      const callId = createCallId()
      const startMs = Date.now()
      try {
        const response = await this.createWithTimeout({
          model: args.model,
          input: [
            {
              role: "system",
              content:
                `${args.systemPrompt}\n\n` +
                "Summarize this mail thread for future turns in under 350 words. " +
                "Prioritize factual points, user goals, constraints, and unresolved questions.",
            },
            {
              role: "system",
              content: `Contact instruction:\n${args.contactInstruction || "(none)"}`,
            },
            {
              role: "user",
              content: args.messages
                .map((message) => `${message.role}: ${message.content}`)
                .join("\n\n"),
            },
          ],
        })

        const summary = this.getOutputText(response).trim()
        if (!summary) {
          throw new Error("Empty summary")
        }
        const durationMs = Date.now() - startMs
        logLine(
          "info",
          `LLM mail summary-generation ${formatLogContext(args.logContext)} model=${args.model} ok ${durationMs}ms attempt=${attempt} cid=${callId}`
        )
        logDebugJson(this.options.debug, {
          event: "llm.call",
          provider: "openai",
          operation: "responses.create",
          component: "mail",
          trigger: "summary-generation",
          ...args.logContext,
          model: args.model,
          status: "ok",
          durationMs,
          attempt,
          timeoutMs: this.options.requestTimeoutMs,
          callId,
        })
        return summary
      } catch (error) {
        const durationMs = Date.now() - startMs
        const details = errorDetails(error)
        logLine(
          "error",
          `LLM mail summary-generation ${formatLogContext(args.logContext)} model=${args.model} error ${durationMs}ms attempt=${attempt} cid=${callId} msg="${details.errorMessage}"`
        )
        logDebugJson(this.options.debug, {
          level: "error",
          event: "llm.call",
          provider: "openai",
          operation: "responses.create",
          component: "mail",
          trigger: "summary-generation",
          ...args.logContext,
          model: args.model,
          status: "error",
          durationMs,
          attempt,
          timeoutMs: this.options.requestTimeoutMs,
          callId,
          errorName: details.errorName,
          errorMessage: details.errorMessage,
        })
        throw error
      }
    })

    return result
  }

  async createImage(args: {
    prompt: string
    modelQuality: "low" | "normal" | "high"
    logContext?: MailLLMLogContext
  }): Promise<{ mimeType: string; binary: Buffer }> {
    const quality = args.modelQuality === "normal" ? "medium" : args.modelQuality
    const callId = createCallId()
    const startMs = Date.now()
    try {
      const image = await this.openaiClient.images.generate({
        model: "gpt-image-1",
        prompt: args.prompt,
        quality,
        size: "1024x1024",
        output_format: "png",
      })

      const item = image.data?.[0]
      if (!item?.b64_json) {
        throw new Error("Image generation returned no binary payload")
      }

      const durationMs = Date.now() - startMs
      logLine(
        "info",
        `LLM mail image-generation ${formatLogContext(args.logContext)} model=gpt-image-1 ok ${durationMs}ms cid=${callId}`
      )
      logDebugJson(this.options.debug, {
        event: "llm.call",
        provider: "openai",
        operation: "images.generate",
        component: "mail",
        trigger: "image-generation",
        ...args.logContext,
        model: "gpt-image-1",
        status: "ok",
        durationMs,
        callId,
      })

      return {
        mimeType: "image/png",
        binary: Buffer.from(item.b64_json, "base64"),
      }
    } catch (error) {
      const durationMs = Date.now() - startMs
      const details = errorDetails(error)
      logLine(
        "error",
        `LLM mail image-generation ${formatLogContext(args.logContext)} model=gpt-image-1 error ${durationMs}ms cid=${callId} msg="${details.errorMessage}"`
      )
      logDebugJson(this.options.debug, {
        level: "error",
        event: "llm.call",
        provider: "openai",
        operation: "images.generate",
        component: "mail",
        trigger: "image-generation",
        ...args.logContext,
        model: "gpt-image-1",
        status: "error",
        durationMs,
        callId,
        errorName: details.errorName,
        errorMessage: details.errorMessage,
      })
      throw error
    }
  }
}

export function createMailLLM(apiKey: string, options?: MailLLMCreateOptions): MailLLM {
  if (process.env.USE_DEV_LLM === "1") {
    return new DevMailLLM()
  }

  if (!apiKey) {
    throw new Error(
      "Missing API key for mail generation. Set YAH_API_KEY or use USE_DEV_LLM=1."
    )
  }

  return new OpenAIMailLLM(new OpenAI({ apiKey }), {
    retryMaxAttempts:
      Number.isInteger(options?.retryMaxAttempts) && (options?.retryMaxAttempts ?? 0) > 0
        ? (options?.retryMaxAttempts as number)
        : 2,
    requestTimeoutMs:
      Number.isInteger(options?.requestTimeoutMs) && (options?.requestTimeoutMs ?? 0) > 0
        ? (options?.requestTimeoutMs as number)
        : 20000,
    maxAttachments:
      Number.isInteger(options?.maxAttachments) && (options?.maxAttachments ?? 0) > 0
        ? (options?.maxAttachments as number)
        : 3,
    maxTextAttachmentChars:
      Number.isInteger(options?.maxTextAttachmentChars) && (options?.maxTextAttachmentChars ?? 0) > 0
        ? (options?.maxTextAttachmentChars as number)
        : 20000,
    debug: isDebugEnabled(options?.debug),
  })
}
